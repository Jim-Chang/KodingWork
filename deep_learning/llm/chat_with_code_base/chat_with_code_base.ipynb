{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wi0R7PXSFRACPcAF4qdXhPO5qCrDXOd4",
      "authorship_tag": "ABX9TyOkyINwmBGAbYkBrC6WmHm0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jim-Chang/KodingWork/blob/master/llm/ask_with_code_base/ask_with_code_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Init SSH key and GenAI\n",
        "# @markdown 於程式碼中填入你的 ssh private key, gemini api key\n",
        "\n",
        "pub_key = \"your pub key (optional)\"\n",
        "# Add Private Key\n",
        "private_key = \"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\n",
        "your private key\n",
        "-----END OPENSSH PRIVATE KEY-----\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "with open(os.path.join(\".\", \"id_rsa\"), \"w\") as f:\n",
        "    f.write(private_key)\n",
        "\n",
        "\n",
        "!rm -Rf ~/.ssh\n",
        "!mkdir ~/.ssh\n",
        "\n",
        "!mv /content/id_rsa ~/.ssh/id_rsa\n",
        "!chmod 600 ~/.ssh/id_rsa\n",
        "\n",
        "!echo -e \"Host gitlab.com\\\\n\\\\tStrictHostKeyChecking no\\\\nHost github.com\\\\n\\\\tStrictHostKeyChecking no\\\\n\" > ~/.ssh/config\n",
        "!chmod 600 ~/.ssh/config\n",
        "\n",
        "\n",
        "import google.generativeai as genai\n",
        "# Add Gemini Api Key\n",
        "genai.configure(api_key=\"your_api_key\")\n"
      ],
      "metadata": {
        "id": "8qxQtjBfEnGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Clone Repo\n",
        "# @markdown #### Input Repo\n",
        "repo_url = \"git@github.com:Jim-Chang/KodingWork.git\" # @param [\"git@github.com:Jim-Chang/KodingWork.git\"] {allow-input: true}\n",
        "# @markdown #### Select Branch (選擇 default 時，使用 repo 的 default branch 設定)\n",
        "branch = \"default\" # @param [\"default\", \"master\", \"main\", \"stage\", \"develop\"] {allow-input: true}\n",
        "\n",
        "target_directory = \"code_base\"\n",
        "git_clone_command = f\"git clone {repo_url} --depth=1 {target_directory}\"\n",
        "\n",
        "!rm -Rf {target_directory}\n",
        "!{git_clone_command}\n",
        "\n",
        "if branch != \"default\":\n",
        "  !cd {target_directory}; git checkout {branch}\n",
        "\n",
        "!cd {target_directory}; git submodule init; git submodule update; git pull"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-jFdr-0vG32j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # 合併 code base 並上傳 GenAI\n",
        "\n",
        "# @markdown ### 勾選要合併的副檔名\n",
        "js = True # @param {type:\"boolean\"}\n",
        "ts = False # @param {type:\"boolean\"}\n",
        "tsx = False # @param {type:\"boolean\"}\n",
        "json = False # @param {type:\"boolean\"}\n",
        "py = False # @param {type:\"boolean\"}\n",
        "\n",
        "file_extensions = []\n",
        "if js:\n",
        "    file_extensions.append(\"js\")\n",
        "if ts:\n",
        "    file_extensions.append(\"ts\")\n",
        "if tsx:\n",
        "    file_extensions.append(\"tsx\")\n",
        "if json:\n",
        "    file_extensions.append(\"json\")\n",
        "if py:\n",
        "    file_extensions.append(\"py\")\n",
        "\n",
        "all_file_contents = \"\"\n",
        "\n",
        "for root, dirs, files in os.walk(target_directory):\n",
        "    for file in files:\n",
        "        if any(file.endswith(ext) for ext in file_extensions):\n",
        "            file_path = os.path.join(root, file)\n",
        "            relative_file_path = os.path.relpath(file_path, target_directory)\n",
        "\n",
        "\n",
        "            with open(file_path, 'r') as f:\n",
        "                file_content = f.read()\n",
        "\n",
        "            all_file_contents += f\"# Source File Path: {relative_file_path}\\n\"\n",
        "            all_file_contents += \"```\\n\"\n",
        "            all_file_contents += file_content\n",
        "            all_file_contents += \"\\n```\\n\"\n",
        "\n",
        "with open(\"all_file_contents.txt\", \"w\") as f:\n",
        "    f.write(all_file_contents)\n",
        "\n",
        "all_file_contents_prompt = genai.upload_file(\"all_file_contents.txt\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nB5sjy2jM_jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7BzuWvFsd8Xt"
      },
      "outputs": [],
      "source": [
        "# @markdown # 設定 Model\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "import hashlib\n",
        "\n",
        "\n",
        "model_name = 'gemini-1.5-pro-latest' # @param [\"gemini-1.5-pro-latest\", \"gemini-1.5-flash-latest\"]\n",
        "temperature = 0.3 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "top_p = 1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "top_k = 32 # @param {type:\"slider\", min:0, max:100, step:1}\n",
        "\n",
        "# Set up the model\n",
        "generation_config = {\n",
        "  \"temperature\": temperature,\n",
        "  \"top_p\": top_p,\n",
        "  \"top_k\": top_k,\n",
        "  \"max_output_tokens\": 8192,\n",
        "}\n",
        "\n",
        "safety_settings = [\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "    \"threshold\": \"BLOCK_NONE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "    \"threshold\": \"BLOCK_NONE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "    \"threshold\": \"BLOCK_NONE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "    \"threshold\": \"BLOCK_NONE\"\n",
        "  },\n",
        "]\n",
        "\n",
        "model = genai.GenerativeModel(model_name=model_name,\n",
        "                              generation_config=generation_config,\n",
        "                              safety_settings=safety_settings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # 初始化/清除 對話紀錄\n",
        "message_history = []"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MRPbks1VBwr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # 輸入 system prompt\n",
        "system_message = \"你是一個資深的軟體工程師，精通後端 python，依照提供的程式碼，解答使用者的問題\" # @param [\"你是一個資深的軟體工程師，精通後端 python，依照提供的程式碼，解答使用者的問題\", \"你是一個資深的軟體工程師，精通前端 react framework\", \"你是一個資深的軟體工程師，精通後端 nest.js，依照提供的程式碼，解答使用者的問題\", \"你是一個資深的軟體工程師，精通後端 node.js，依照提供的程式碼，解答使用者的問題\"] {allow-input: true}\n",
        "\n",
        "code_prompts = [\n",
        "    \"程式碼是由多個檔案組成，每份檔案的內容由 code block 包夾，每個 code block 起始處前一行會提供該檔案的路徑與檔名，回覆時請務必依照檔案對應的檔名提供給使用者\",\n",
        "    \"## Source Code Begin ##\",\n",
        "    all_file_contents_prompt,\n",
        "    \"## Source Code End ##\",\n",
        "]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8sL09yJ-01ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # 對話詢問\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "\n",
        "\n",
        "user_message = \"\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "def print_separate_line():\n",
        "  print(\"====================================================================================\")\n",
        "\n",
        "\n",
        "print(\"## Message History ##\")\n",
        "print_separate_line()\n",
        "for message in message_history:\n",
        "  if message.startswith(\"ai:\"):\n",
        "    display(Markdown(message))\n",
        "  else:\n",
        "    print(message)\n",
        "  print_separate_line()\n",
        "\n",
        "if user_message:\n",
        "  print(f\"user:\\n{user_message}\")\n",
        "  print_separate_line()\n",
        "\n",
        "print(\"## AI Generating...\")\n",
        "print_separate_line()\n",
        "\n",
        "\n",
        "prompt_parts = [\n",
        "    system_message,\n",
        "    *code_prompts,\n",
        "    *message_history,\n",
        "]\n",
        "\n",
        "if user_message:\n",
        "  prompt_parts.append(f\"user:\\n{user_message}\")\n",
        "\n",
        "streaming = True # @param {type:\"boolean\"}\n",
        "display_handle = display(Markdown(\"\"), display_id=True)\n",
        "ai_result = \"\"\n",
        "\n",
        "if streaming:\n",
        "  response = model.generate_content(prompt_parts, stream=True, request_options={'timeout': 600})\n",
        "  for chunk in response:\n",
        "    ai_result += chunk.text\n",
        "    display_handle.update(Markdown(ai_result))\n",
        "\n",
        "else:\n",
        "  response = model.generate_content(prompt_parts)\n",
        "  ai_result = response.text\n",
        "  display(Markdown(ai_result))\n",
        "\n",
        "if user_message:\n",
        "  message_history.append(f\"user:\\n{user_message}\")\n",
        "\n",
        "message_history.append(f\"ai:\\n{ai_result}\")\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lPitmHEBfXbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}